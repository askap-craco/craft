{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline FMDT one sample at a time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name config",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-56eb0be81b95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfdmt\u001b[0m \u001b[0;31m# you'll need to have ../python in  you PYTHONPATH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgraphviz\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDigraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/FRIGG_2/ban115/craft/python/fdmt.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumba\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnjit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/FRIGG_2/ban115/craftenv/local/lib/python2.7/site-packages/numba/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mget_versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_runtests\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mruntests\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Re-export typeof\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name config"
     ]
    }
   ],
   "source": [
    "from pylab import *\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import calc11\n",
    "import numpy as np\n",
    "from scipy import constants\n",
    "import fdmt # you'll need to have ../python in  you PYTHONPATH\n",
    "from graphviz import Digraph\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = 1.12 # center frequency GHz\n",
    "bw = 0.288 # bandwidth GHz\n",
    "Nd = 512 # number of DM trials\n",
    "Nchan= 256\n",
    "Nt = 256 # time block size\n",
    "Tint = 0.864e-3 # integration time - seconds\n",
    "Npol = 2 # input number of polarisations\n",
    "Npix = 256\n",
    "ignore_ant = ['ak31','ak32','ak33','ak34','ak35','ak36']\n",
    "f1 = fc - bw/2.\n",
    "f2 = fc + bw/2.\n",
    "chanbw = 1e-3\n",
    "lam1 = constants.c/f1/1e9\n",
    "lam2 = constants.c/f2/1e9\n",
    "freqs = f1 + np.arange(Nchan)*chanbw\n",
    "lambdas = constants.c / (freqs*1e9)\n",
    "nbytes = 2\n",
    "\n",
    "\n",
    "print lam1, lam2, lambdas.min(), lambdas.max(), freqs.min(), freqs.max()\n",
    "configs = ((2,2), (4, 4), (8,8), (16,16), (32,32), (64,64),(128,128),(256,256),(1024,256),(1024,288),(256,288),(32,288))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thefdmt = fdmt.Fdmt(f1, chanbw, Nchan, Nd, Nt)\n",
    "print(dir(thefdmt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmt(i,c,d):\n",
    "    return 'I{}D{}C{}'.format(i,d,c)\n",
    "\n",
    "maindot = Digraph(comment='FDMT', format='png')\n",
    "all_offsets = []\n",
    "nnodes = 0\n",
    "nplotnodes = 0\n",
    "output_fifo_sizes = {}\n",
    "\n",
    "# make initial nodes\n",
    "ishape = thefdmt.hist_state_shape[0]\n",
    "dot = Digraph(name='cluster_iter0')\n",
    "dot.attr(label='Iteration 0')\n",
    "maindot.subgraph(dot)\n",
    "for c in xrange(ishape[0]):\n",
    "    for d in xrange(ishape[1]):\n",
    "        maindot.node(fmt(0, c, d))\n",
    "        nnodes += 1\n",
    "        nplotnodes += 1\n",
    "\n",
    "        \n",
    "for iterno, nfd in enumerate(thefdmt.hist_nf_data):\n",
    "    out_shape = thefdmt.hist_state_shape[iterno+1]\n",
    "    in_shape = thefdmt.hist_state_shape[iterno]\n",
    "    nchan, ndt, nt_out = out_shape\n",
    "    print 'Iteration {} in={}={} out={}={}'.format(iterno, in_shape, in_shape[0:2].prod(), out_shape, out_shape[:2].prod())\n",
    "    dot = Digraph(name='cluster_iter{}'.format(iterno+1))\n",
    "    dot.attr(label='Iteration {}'.format(iterno+1))\n",
    "    maindot.subgraph(dot)\n",
    "\n",
    "    for ochan in xrange(nchan):\n",
    "        chanconfig = thefdmt.hist_nf_data[iterno][ochan][-1]\n",
    "        #print '\\tOut channel {}'.format(ochan)\n",
    "        last_id1 = -1\n",
    "        last_id2 = -1\n",
    "        for idt, config in enumerate(chanconfig):\n",
    "            _, id1, offset, id2, _, _, _ = config\n",
    "            do_copy = id2 == -1\n",
    "            inchan1 = 2*ochan\n",
    "            inchan2 = inchan1+1\n",
    "            id1_hit = ''\n",
    "            id2_hit = ''\n",
    "            if last_id1 == id1:\n",
    "                id1_hit = '*'\n",
    "            if last_id2 == id2:\n",
    "                id2_hist = '*'\n",
    "            \n",
    "            #print '\\t Out Channel {} idout={} from chan{}/idt{}{} + chan{}/idt{}{} with offset {}' \\\n",
    "            #    .format(ochan, idt, inchan1, id1, id1_hit, inchan2, id2, id2_hit, offset)\n",
    "            \n",
    "            all_offsets.append(offset)\n",
    "            style = 'dashed' if do_copy else 'solid'\n",
    "            nnodes += 1\n",
    "            label = None if offset == 0 else 'd={}'.format(offset)\n",
    "            n1 = fmt(iterno, inchan1, id1)\n",
    "            n2 = fmt(iterno, inchan2, id2)\n",
    "            nout = fmt(iterno+1,ochan, idt)\n",
    "            \n",
    "            # For a given sum - only maintain 1 fifo of the given max length\n",
    "            ff = output_fifo_sizes.get(n2, [])\n",
    "            ff.append(offset)\n",
    "            output_fifo_sizes[n2] = ff\n",
    "            \n",
    "            #addnode = iterno >= 6 and idt > thefdmt.hist_state_shape[iterno+1][1]*0.9\n",
    "            addnode = True\n",
    "            if addnode:\n",
    "                maindot.node(fmt(iterno+1,ochan, idt), style=style)\n",
    "                nplotnodes += 1\n",
    "                maindot.edge(n1, nout)\n",
    "                if not do_copy:\n",
    "                    color = 'black' if offset == 0 else 'red'\n",
    "                    maindot.edge(n2, nout,label=label, color=color)\n",
    "    \n",
    "            last_id1 = id1\n",
    "            last_id2 = id2\n",
    "\n",
    "    \n",
    "nfifo_outputs = {k:max(ff) for k, ff in output_fifo_sizes.iteritems()}\n",
    "bulk_fifo_sizes = {k:min(ff) for k, ff in output_fifo_sizes.iteritems()}\n",
    "fanout_fifo_sizes = {k:max(ff) - min(ff) for k, ff in output_fifo_sizes.iteritems()}\n",
    "\n",
    "print 'Total offsets', sum(all_offsets), 'nfifo outputs', sum(nfifo_outputs.values()), 'largest fifo', max(nfifo_outputs.values()), 'total nodes', nnodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'nplotnodes', nplotnodes\n",
    "if nplotnodes < 1200:\n",
    "    maindot.view()\n",
    "    \n",
    "print all_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "total_offsets = np.array(nfifo_outputs.values())\n",
    "bulk_sizes = np.array(bulk_fifo_sizes.values())\n",
    "fanout_sizes = np.array(fanout_fifo_sizes.values())\n",
    "\n",
    "def print_sr_stats(all_offsets):\n",
    "    print \"num FIFOs\", len(all_offsets)\n",
    "    print 'Total SR entries', sum(all_offsets)\n",
    "    print 'Number of SR length ==0', sum(all_offsets == 0)\n",
    "    print 'Number of SR length ==1', sum(all_offsets == 1)\n",
    "    print 'Number of SR length ==2', sum(all_offsets == 2)\n",
    "    print 'Number of SR length >=1', sum(all_offsets >= 1)\n",
    "    print 'Number of SR length <=16', sum(all_offsets <= 16)\n",
    "    print 'Number of SR length <=32', sum(all_offsets <= 32)\n",
    "    print 'Number of SR length <=64', sum(all_offsets <= 64)\n",
    "    print 'Number of SR length >64', sum(all_offsets >64)\n",
    "    print 'Number of SR length >128', sum(all_offsets >128)\n",
    "    print 'Number of SR length >256', sum(all_offsets >256)\n",
    "    print 'Number of SR length >512', sum(all_offsets >512)\n",
    "\n",
    "    print 'Max SR length', max(all_offsets)\n",
    "\n",
    "print '*'*8, 'total'\n",
    "print_sr_stats(total_offsets)\n",
    "\n",
    "print '*'*8, 'bulk'\n",
    "print_sr_stats(bulk_sizes)\n",
    "\n",
    "print '*'*8, 'fanout'\n",
    "print_sr_stats(fanout_sizes)\n",
    "\n",
    "x = hist(total_offsets, np.arange(0, max(total_offsets), 1), log=True, histtype='step')\n",
    "xlabel('Total shift register size')\n",
    "ylabel('Number of SRs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Total offsets', sum(total_offsets), ' total sum operations', len(all_offsets)\n",
    "hist(total_offsets, np.arange(0, max(total_offsets), 1), log=False, cumulative=True, histtype='step')\n",
    "xlabel('FIFO size (elements)')\n",
    "ylabel('Cumulative number of FIFOs')\n",
    "\n",
    "\n",
    "#xlim(0, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Total offsets', sum(total_offsets), ' total sum operations', len(all_offsets)\n",
    "hist(total_offsets, np.arange(0, max(total_offsets), 1), log=True, cumulative=False, histtype='step')\n",
    "xlabel('FIFO size (elements)')\n",
    "ylabel('Number of FIFOs')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(x[1][0:-1], x[0]*(x[1][0:-1]))\n",
    "xlabel('FIFO size (elements)')\n",
    "ylabel('Total number of elements in fifos of this size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(x[1][0:-1], np.cumsum(x[0]*(x[1][0:-1])))\n",
    "xlabel('FIFO size (elements)')\n",
    "ylabel('Total number of elements in fifos < this size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_outputs = [len(ff) for ff in output_fifo_sizes.values()]\n",
    "max_ff_length = [max(ff) for ff in output_fifo_sizes.values()]\n",
    "ff_length_range = [max(ff) - min(ff) for ff in output_fifo_sizes.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist(num_outputs, np.arange(0, max(num_outputs)+3) - 0.5, log=True)\n",
    "xlabel('Number of FIFO outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist(max_ff_length, np.arange(0, max(max_ff_length)+3) - 0.5, log=True)\n",
    "xlabel('Maximum fifo length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist(ff_length_range, np.arange(0, max(ff_length_range)+3) - 0.5, log=True)\n",
    "xlabel('Number of FIFO taps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def make_c_file(thefdmt):\n",
    "    all_offsets = []\n",
    "    output_fifo_sizes = OrderedDict()\n",
    "    # make initial nodes\n",
    "    preamble = '''\n",
    "#ifndef _FDMT_PROCESS_H\n",
    "#define _FDMT_PROCESS_H\n",
    "    // \n",
    "// FDMT produced by FDMT one samlpe at a time.ipynb\n",
    "// nd={f.max_dt} nf={f.n_f} fmin={f.f_min} nchan={f.n_f} df={f.d_f} bw={f.bw}\n",
    "//\n",
    "\n",
    "const int NC={f.n_f}; // Number of channels\n",
    "const float FMIN = {f.f_min}; // Frequency of bottom channel (GHz)\n",
    "const float FMAX = {f.f_max}; // Frequency of bottom channel (GHz)\n",
    "const int ND = {f.max_dt}; // Number of output DM trials\n",
    "const int ND_IN = {f.hist_state_shape[0][1]}; // number of input dm trials\n",
    "const float DF = {f.d_f}; // channel interval (GHz)\n",
    "const float BW = {f.bw}; // Total bandwidth (GHz)\n",
    "\n",
    "'''.format(f=thefdmt)\n",
    "    ishape = thefdmt.hist_state_shape[0]\n",
    "    read = '// Read inputs\\n'\n",
    "    for c in xrange(ishape[0]):\n",
    "        for d in xrange(ishape[1]):\n",
    "            read += 'fdmt_t {} = in[{}][{}];\\n'.format(fmt(0, c, d), d,c)\n",
    "\n",
    "\n",
    "    queuedecl = '// FIFO declarations\\n\\n'\n",
    "    queuepush = '// FIFO push statements\\n\\n'\n",
    "    do_sums = ''\n",
    "    for iterno, nfd in enumerate(thefdmt.hist_nf_data):\n",
    "        out_shape = thefdmt.hist_state_shape[iterno+1]\n",
    "        in_shape = thefdmt.hist_state_shape[iterno]\n",
    "        nchan, ndt, nt_out = out_shape\n",
    "        print 'Iteration {} in={} size={} out={} size={}'.format(iterno+1, in_shape[0:2], in_shape[0:2].prod(), out_shape[0:2], out_shape[:2].prod())\n",
    "        \n",
    "        do_sums += '\\n\\n// Iteration {} in={} size={} out={} size={} \\n\\n'.format(iterno+1, in_shape, in_shape[0:2].prod(), out_shape, out_shape[:2].prod())\n",
    "        \n",
    "        for ochan in xrange(nchan):\n",
    "            chanconfig = thefdmt.hist_nf_data[iterno][ochan][-1]\n",
    "            #print '\\tOut channel {}'.format(ochan)\n",
    "            last_id1 = -1\n",
    "            last_id2 = -1\n",
    "            \n",
    "            do_sums += '\\n // Output channel {}\\n'.format(ochan)\n",
    "            for idt, config in enumerate(chanconfig):\n",
    "                _, id1, offset, id2, _, _, _ = config\n",
    "                do_copy = id2 == -1\n",
    "                inchan1 = 2*ochan\n",
    "                inchan2 = inchan1+1\n",
    "\n",
    "                #print '\\t Out Channel {} idout={} from chan{}/idt{}{} + chan{}/idt{}{} with offset {}' \\\n",
    "                #    .format(ochan, idt, inchan1, id1, id1_hit, inchan2, id2, id2_hit, offset)\n",
    "\n",
    "                all_offsets.append(offset)\n",
    "\n",
    "                n1 = fmt(iterno, inchan1, id1)\n",
    "                n2 = fmt(iterno, inchan2, id2)\n",
    "                nout = fmt(iterno+1,ochan, idt)\n",
    "                \n",
    "                # For a given sum - only maintain 1 fifo of the given max length\n",
    "                ff = output_fifo_sizes.get(n2, [])\n",
    "                ff.append(offset)\n",
    "                output_fifo_sizes[n2] = ff\n",
    "                \n",
    "                \n",
    "                if do_copy:\n",
    "                    do_sums += 'fdmt_t {} = {};\\n'.format(nout, n1)\n",
    "                else:\n",
    "                    if offset == 0:\n",
    "                        do_sums += 'fdmt_t {} = {} + {};\\n'.format(nout, n1, n2)\n",
    "                    else:\n",
    "                        do_sums += 'fdmt_t {} = {} + {}_fifo.read({});\\n'.format(nout, n1, n2, offset)\n",
    "                        \n",
    "\n",
    "    for infmt, ff_sizes in output_fifo_sizes.iteritems():\n",
    "        #queuedecl += 'static fdmt_fifo<{},{}> {}_fifo;\\n'.format(min(ff_sizes), max(ff_sizes), infmt)\n",
    "        #queuedecl += '#pragma HLS ARRAY_PARTITION variable={}_fifo dim=1 complete\\n'.format(infmt)\n",
    "        # add 1 because of how lengths are defined. Ap_shift_reg wants size, fdmt_fifo wanted max_read_value\n",
    "        queuedecl += 'static ap_shift_reg<fdmt_t, {}> {}_fifo;\\n'.format(max(ff_sizes)+1, infmt);\n",
    "        queuepush += '{}_fifo.shift({});\\n'.format(infmt, infmt)\n",
    "    \n",
    "    # write outputs\n",
    "    oshape = thefdmt.hist_state_shape[iterno+1]\n",
    "\n",
    "    write = '\\n\\n// Write outputs\\n\\n'\n",
    "    for c in xrange(oshape[0]):\n",
    "        for d in xrange(oshape[1]):\n",
    "            write += 'out[{}] = {};\\n'.format(d, fmt(iterno+1, c, d))\n",
    "\n",
    "                \n",
    "    funcstart = '''\n",
    "    void fdmt_process(fdmt_t in[ND_IN][NCHAN], fdmt_t out[ND]) {\n",
    "    \n",
    "    #pragma HLS PIPELINE II=16\n",
    "    '''\n",
    "    funcend = '''\n",
    "    }\n",
    "    \n",
    "#\n",
    "    '''\n",
    "    \n",
    "    fileend = \"#endif\"\n",
    "    cfile = preamble  + queuedecl + read + do_sums + queuepush + write + fileend\n",
    "    \n",
    "    return cfile\n",
    "\n",
    "\n",
    "s = make_c_file(thefdmt)\n",
    "fout = 'fdmt_d{f.max_dt}_c{f.n_f}_f{f.f_min}.h'.format(f=thefdmt)\n",
    "print('Writing to ', fout)\n",
    "#print(s)\n",
    "with open(fout, 'w') as fout:\n",
    "    fout.write(s)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nd, nchan in configs:\n",
    "    thefdmt = fdmt.Fdmt(f1, chanbw, nchan, nd, Nt)\n",
    "    s = make_c_file(thefdmt)\n",
    "    fout = 'fdmt_d{f.max_dt}_c{f.n_f}_f{f.f_min}.h'.format(f=thefdmt)\n",
    "    print('Writing to ', fout)\n",
    "    #print(s)\n",
    "    with open(fout, 'w') as fout:\n",
    "        fout.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def make_c_file_iter(thefdmt):\n",
    "    all_offsets = []\n",
    "    output_fifo_sizes = OrderedDict()\n",
    "    # make initial nodes\n",
    "    preamble = '''\n",
    "#ifndef _FDMT_PROCESS_H\n",
    "#define _FDMT_PROCESS_H\n",
    "    // \n",
    "// FDMT produced by FDMT one samlpe at a time.ipynb\n",
    "// nd={f.max_dt} nf={f.n_f} fmin={f.f_min} nchan={f.n_f} df={f.d_f} bw={f.bw}\n",
    "//\n",
    "\n",
    "const int NC={f.n_f}; // Number of channels\n",
    "const float FMIN = {f.f_min}; // Frequency of bottom channel (GHz)\n",
    "const float FMAX = {f.f_max}; // Frequency of bottom channel (GHz)\n",
    "const int ND = {f.max_dt}; // Number of output DM trials\n",
    "const int ND_IN = {f.hist_state_shape[0][1]}; // number of input dm trials\n",
    "const float DF = {f.d_f}; // channel interval (GHz)\n",
    "const float BW = {f.bw}; // Total bandwidth (GHz)\n",
    "\n",
    "'''.format(f=thefdmt)\n",
    "    ishape = thefdmt.hist_state_shape[0]\n",
    "    \n",
    "    iters = ''\n",
    "    \n",
    "\n",
    "    for iterno, nfd in enumerate(thefdmt.hist_nf_data):\n",
    "        out_shape = thefdmt.hist_state_shape[iterno+1]\n",
    "        in_shape = thefdmt.hist_state_shape[iterno]\n",
    "        nchan, ndt, nt_out = out_shape\n",
    "\n",
    "        print 'Iteration {} in={} size={} out={} size={}'.format(iterno+1, in_shape[0:2], in_shape[0:2].prod(), out_shape[0:2], out_shape[:2].prod())\n",
    "        ncout, ndout = out_shape[0:2]\n",
    "        ncin, ndin = in_shape[0:2]\n",
    "        sums_done = set()\n",
    "        queuedecl = ''\n",
    "        queuedecl += '//Iteration {iterno}\\n'.format(**locals())\n",
    "        if ncout == 1:\n",
    "            queuedecl += 'void iteration{iterno}(const fdmt_t in[{ndin}][{ncin}], fdmt_t out[{ndout}]) \\n'.format(**locals())\n",
    "        else:\n",
    "            queuedecl += 'void iteration{iterno}(const fdmt_t in[{ndin}][{ncin}], fdmt_t out[{ndout}][{ncout}]) \\n'.format(**locals())\n",
    "\n",
    "        queuedecl += '{\\n'\n",
    "        queuedecl += '// FIFO declarations\\n\\n'\n",
    "        queuepush = '// FIFO push statements\\n\\n'\n",
    "        do_sums = ''\n",
    "        #do_sums += '\\n\\n// Iteration {} in={} size={} out={} size={} \\n\\n'.format(iterno+1, in_shape, in_shape[0:2].prod(), out_shape, out_shape[:2].prod())\n",
    "        read = ' '*4 + '// Read inputs\\n'\n",
    "        for c in xrange(ncin):\n",
    "            for d in xrange(ndin):\n",
    "                read += '    fdmt_t {} = in[{}][{}];\\n'.format(fmt(iterno, c, d), d,c)\n",
    "                \n",
    "\n",
    "        for ochan in xrange(nchan):\n",
    "            chanconfig = thefdmt.hist_nf_data[iterno][ochan][-1]\n",
    "            #print '\\tOut channel {}'.format(ochan)\n",
    "            last_id1 = -1\n",
    "            last_id2 = -1\n",
    "            \n",
    "            do_sums += '\\n // Output channel {}\\n'.format(ochan)\n",
    "            for idt, config in enumerate(chanconfig):\n",
    "                _, id1, offset, id2, _, _, _ = config\n",
    "                do_copy = id2 == -1\n",
    "                inchan1 = 2*ochan\n",
    "                inchan2 = inchan1+1\n",
    "\n",
    "                #print '\\t Out Channel {} idout={} from chan{}/idt{}{} + chan{}/idt{}{} with offset {}' \\\n",
    "                #    .format(ochan, idt, inchan1, id1, id1_hit, inchan2, id2, id2_hit, offset)\n",
    "\n",
    "                all_offsets.append(offset)\n",
    "\n",
    "                n1 = fmt(iterno, inchan1, id1)\n",
    "                n2 = fmt(iterno, inchan2, id2)\n",
    "                nout = fmt(iterno+1,ochan, idt)\n",
    "                \n",
    "                # For a given sum - only maintain 1 fifo of the given max length\n",
    "                ff = output_fifo_sizes.get(n2, [])\n",
    "                ff.append(offset)\n",
    "                output_fifo_sizes[n2] = ff\n",
    "                \n",
    "                \n",
    "                if do_copy:\n",
    "                    do_sums += 'fdmt_t {} = {};\\n'.format(nout, n1)\n",
    "                else:\n",
    "                    if offset == 0:\n",
    "                        do_sums += '    fdmt_t {} = {} + {};\\n'.format(nout, n1, n2)\n",
    "                    else:\n",
    "                        do_sums += '    fdmt_t {} = {} + {}_fifo.read({});\\n'.format(nout, n1, n2, offset)\n",
    "                        \n",
    "                sums_done.add((ochan, idt))\n",
    "                        \n",
    "\n",
    "        # Find FIFOS for this iteration\n",
    "        myfifos = filter(lambda f: f.startswith('I{}'.format(iterno)), output_fifo_sizes)\n",
    "        for infmt in myfifos:\n",
    "            ff_sizes = output_fifo_sizes[infmt]\n",
    "            #queuedecl += 'static fdmt_fifo<{},{}> {}_fifo;\\n'.format(min(ff_sizes), max(ff_sizes), infmt)\n",
    "            #queuedecl += '#pragma HLS ARRAY_PARTITION variable={}_fifo dim=1 complete\\n'.format(infmt)\n",
    "            # add 1 because of how lengths are defined. Ap_shift_reg wants size, fdmt_fifo wanted max_read_value\n",
    "            queuedecl += '    static ap_shift_reg<fdmt_t, {}> {}_fifo;\\n'.format(max(ff_sizes)+1, infmt);\n",
    "            queuepush += '    {}_fifo.shift({});\\n'.format(infmt, infmt)\n",
    "\n",
    "        # write outputs\n",
    "        oshape = thefdmt.hist_state_shape[iterno+1]\n",
    "\n",
    "        write = '\\n\\n// Write outputs\\n\\n'\n",
    "        for c in xrange(ncout):\n",
    "            for d in xrange(ndout):\n",
    "                if (c, d) in sums_done: # if the sum was actually done - load it into the array\n",
    "                    write += '    out[{}][{}] = {};\\n'.format(d, c , fmt(iterno+1, c, d))\n",
    "                else:\n",
    "                    write += '    out[{}][{}] = {};\\n'.format(d, c , 0) # set the few dangling outputs to zero\n",
    "                \n",
    "        \n",
    "        iters += queuedecl + read  + do_sums + queuepush + write + '}\\n\\n'\n",
    "                \n",
    "    funcstart = '''void fdmt_process(fdmt_t in[ND_IN][NC], fdmt_t out[ND]) { \n",
    "    #pragma HLS PIPELINE II=16\n",
    "'''\n",
    "    funcdecl = ''\n",
    "    funcrun = ''\n",
    "    lastiter = len(thefdmt.hist_nf_data) -1\n",
    "    for iterno, nfd in enumerate(thefdmt.hist_nf_data):\n",
    "        nextiter = iterno+1\n",
    "        out_shape = thefdmt.hist_state_shape[nextiter]\n",
    "        in_shape = thefdmt.hist_state_shape[iterno]\n",
    "        if iterno != 0:\n",
    "            funcdecl += '    fdmt_t d_iter{i}[{nd}][{nc}];\\n'.format(i=iterno,nc=in_shape[0], nd=in_shape[1])\n",
    "            \n",
    "        if iterno == 0:\n",
    "            funcrun += '    iteration{iterno}(in, d_iter{nextiter});\\n'.format(**locals())\n",
    "        elif iterno == lastiter:\n",
    "            funcrun += '    iteration{iterno}(d_iter{iterno}, out);\\n'.format(**locals())\n",
    "        else:\n",
    "            funcrun += '    iteration{iterno}(d_iter{iterno}, d_iter{nextiter});\\n'.format(**locals())\n",
    "\n",
    "    \n",
    "    funcend = '''\n",
    "}\n",
    "'''\n",
    "    \n",
    "    fileend = \"#endif\"\n",
    "    cfile = preamble  + iters + funcstart + funcdecl + funcrun + funcend + fileend\n",
    "    \n",
    "    return cfile\n",
    "\n",
    "\n",
    "thefdmt = fdmt.Fdmt(f1, chanbw, 32, 32, Nt)\n",
    "\n",
    "s = make_c_file_iter(thefdmt)\n",
    "fout = 'fdmt_d{f.max_dt}_c{f.n_f}_f{f.f_min}_iter.h'.format(f=thefdmt)\n",
    "print('Writing to ', fout)\n",
    "print(s)\n",
    "with open(fout, 'w') as fout:\n",
    "    fout.write(s)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nd, nchan in configs:\n",
    "    thefdmt = fdmt.Fdmt(f1, chanbw, nchan, nd, Nt)\n",
    "    s = make_c_file_iter(thefdmt)\n",
    "    fout = 'fdmt_d{f.max_dt}_c{f.n_f}_f{f.f_min}_iter.h'.format(f=thefdmt)\n",
    "    print('Writing to ', fout)\n",
    "    #print(s)\n",
    "    with open(fout, 'w') as fout:\n",
    "        fout.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def make_c_file_iter2(thefdmt, fifos_per_group=64):\n",
    "    all_offsets = []\n",
    "    output_fifo_sizes = OrderedDict()\n",
    "    # make initial nodes\n",
    "    preamble = '''\n",
    "#ifndef _FDMT_PROCESS_H\n",
    "#define _FDMT_PROCESS_H\n",
    "    // \n",
    "// FDMT produced by FDMT one samlpe at a time.ipynb\n",
    "// nd={f.max_dt} nf={f.n_f} fmin={f.f_min} nchan={f.n_f} df={f.d_f} bw={f.bw}\n",
    "//\n",
    "\n",
    "const int NC={f.n_f}; // Number of channels\n",
    "const float FMIN = {f.f_min}; // Frequency of bottom channel (GHz)\n",
    "const float FMAX = {f.f_max}; // Frequency of bottom channel (GHz)\n",
    "const int ND = {f.max_dt}; // Number of output DM trials\n",
    "const int ND_IN = {f.hist_state_shape[0][1]}; // number of input dm trials\n",
    "const float DF = {f.d_f}; // channel interval (GHz)\n",
    "const float BW = {f.bw}; // Total bandwidth (GHz)\n",
    "\n",
    "'''.format(f=thefdmt)\n",
    "    ishape = thefdmt.hist_state_shape[0]\n",
    "    \n",
    "    iters = ''\n",
    "    \n",
    "    queuedecl = ''\n",
    "    \n",
    "\n",
    "    for iterno, nfd in enumerate(thefdmt.hist_nf_data):\n",
    "        out_shape = thefdmt.hist_state_shape[iterno+1]\n",
    "        in_shape = thefdmt.hist_state_shape[iterno]\n",
    "        nchan, ndt, nt_out = out_shape\n",
    "\n",
    "        print 'Iteration {} in={} size={} out={} size={}'.format(iterno+1, in_shape[0:2], in_shape[0:2].prod(), out_shape[0:2], out_shape[:2].prod())\n",
    "        ncout, ndout = out_shape[0:2]\n",
    "        ncin, ndin = in_shape[0:2]\n",
    "        sums_done = set()\n",
    "        iterstart = ''\n",
    "        iterstart += '//Iteration {iterno}\\n'.format(**locals())\n",
    "        if ncout == 1:\n",
    "            iterstart += 'void iteration{iterno}(unsigned int t, const fdmt_t in[{ndin}][{ncin}], fdmt_t out[{ndout}]) \\n'.format(**locals())\n",
    "        else:\n",
    "            iterstart += 'void iteration{iterno}(unsigned int t, const fdmt_t in[{ndin}][{ncin}], fdmt_t out[{ndout}][{ncout}]) \\n'.format(**locals())\n",
    "\n",
    "        iterstart += '{\\n'\n",
    "        queuepush = '// FIFO push statements\\n\\n'\n",
    "        do_sums = ''\n",
    "        #do_sums += '\\n\\n// Iteration {} in={} size={} out={} size={} \\n\\n'.format(iterno+1, in_shape, in_shape[0:2].prod(), out_shape, out_shape[:2].prod())\n",
    "        read = ' '*4 + '// Read inputs\\n'\n",
    "        for c in xrange(ncin):\n",
    "            for d in xrange(ndin):\n",
    "                read += '    fdmt_t {} = in[{}][{}];\\n'.format(fmt(iterno, c, d), d,c)\n",
    "                \n",
    "\n",
    "        for ochan in xrange(nchan):\n",
    "            chanconfig = thefdmt.hist_nf_data[iterno][ochan][-1]\n",
    "            #print '\\tOut channel {}'.format(ochan)\n",
    "            last_id1 = -1\n",
    "            last_id2 = -1\n",
    "            \n",
    "            do_sums += '\\n // Output channel {}\\n'.format(ochan)\n",
    "            for idt, config in enumerate(chanconfig):\n",
    "                _, id1, offset, id2, _, _, _ = config\n",
    "                do_copy = id2 == -1\n",
    "                inchan1 = 2*ochan\n",
    "                inchan2 = inchan1+1\n",
    "\n",
    "                #print '\\t Out Channel {} idout={} from chan{}/idt{}{} + chan{}/idt{}{} with offset {}' \\\n",
    "                #    .format(ochan, idt, inchan1, id1, id1_hit, inchan2, id2, id2_hit, offset)\n",
    "\n",
    "                all_offsets.append(offset)\n",
    "\n",
    "                n1 = fmt(iterno, inchan1, id1)\n",
    "                n2 = fmt(iterno, inchan2, id2)\n",
    "                nout = fmt(iterno+1,ochan, idt)\n",
    "                \n",
    "                # For a given sum - only maintain 1 fifo of the given max length\n",
    "                ff = output_fifo_sizes.get(n2, [])\n",
    "                ff.append(offset)\n",
    "                output_fifo_sizes[n2] = ff\n",
    "                \n",
    "                \n",
    "                if do_copy:\n",
    "                    do_sums += 'fdmt_t {} = {};\\n'.format(nout, n1)\n",
    "                else:\n",
    "                    if offset == 0:\n",
    "                        do_sums += '    fdmt_t {} = {} + {};\\n'.format(nout, n1, n2)\n",
    "                    else:\n",
    "                        do_sums += '    fdmt_t {} = {} + {}_fifo.read_blank(t, {});\\n'.format(nout, n1, n2, offset-1)\n",
    "                        \n",
    "                sums_done.add((ochan, idt))\n",
    "                        \n",
    "\n",
    "        # Find FIFOS for this iteration\n",
    "        myfifos = filter(lambda f: f.startswith('I{}'.format(iterno)), output_fifo_sizes)\n",
    "        for infmt in myfifos:\n",
    "            ff_sizes = output_fifo_sizes[infmt]\n",
    "            #queuedecl += 'static fdmt_fifo<{},{}> {}_fifo;\\n'.format(min(ff_sizes), max(ff_sizes), infmt)\n",
    "            #queuedecl += '#pragma HLS ARRAY_PARTITION variable={}_fifo dim=1 complete\\n'.format(infmt)\n",
    "            # add 1 because of how lengths are defined. Ap_shift_reg wants size, fdmt_fifo wanted max_read_value\n",
    "            queuepush += '    {}_fifo.shift({});\\n'.format(infmt, infmt)\n",
    "\n",
    "        # write outputs\n",
    "        oshape = thefdmt.hist_state_shape[iterno+1]\n",
    "\n",
    "        write = '\\n\\n// Write outputs\\n\\n'\n",
    "        for c in xrange(ncout):\n",
    "            for d in xrange(ndout):\n",
    "                if (c, d) in sums_done: # if the sum was actually done - load it into the array\n",
    "                    vout = fmt(iterno+1, c, d);\n",
    "                else:\n",
    "                    vout = '0';  # set the few dangling outputs to zero\n",
    "                \n",
    "                if ncout == 1:                    \n",
    "                    write += '    out[{}] = {};\\n'.format(d, vout)\n",
    "                else:\n",
    "                    write += '    out[{}][{}] = {};\\n'.format(d, c , vout)\n",
    "                \n",
    "        \n",
    "        iters += iterstart + read  + do_sums + queuepush + write + '}\\n\\n'\n",
    "        \n",
    "    # sort queues by size\n",
    "    sorted_queues = sorted(output_fifo_sizes.items(), key=lambda fsz: max(fsz[1]))\n",
    "    nfifos = len(output_fifo_sizes)\n",
    "    ngroups = int(np.ceil(float(nfifos)/float(fifos_per_group)))\n",
    "    npad = ngroups*fifos_per_group - nfifos # number of FIFOS not to include in the first group\n",
    "    assert 0 <= npad < fifos_per_group\n",
    "    # NB: padding at the beginning rather than the end saves a bunch of memory\n",
    "    group_sizes = np.zeros(ngroups, dtype=int)\n",
    "    group_fifos = {}\n",
    "    for fifo_enum, (fifo_name, fifo_sizes) in enumerate(sorted_queues):\n",
    "        fifo_id = fifo_enum + npad # pad id from the beginning\n",
    "        group_id = fifo_id // fifos_per_group\n",
    "        group_offset = fifo_id % fifos_per_group\n",
    "        fifo_size = max(fifo_sizes)\n",
    "        queuedecl += 'static FdmtFifo<{}, {}, {}> {}_fifo;\\n'.format(fifo_size, group_id, group_offset, fifo_name);\n",
    "        group_sizes[group_id] = max(group_sizes[group_id], fifo_size)\n",
    "        gf = group_fifos.get(group_id, [])\n",
    "        gf.append(fifo_name)\n",
    "        group_fifos[group_id] = gf\n",
    "        \n",
    "\n",
    "    group_sizes_csep = ','.join(map(str, group_sizes))\n",
    "    group_offsets_csep =  ','.join(map(str, np.cumsum(group_sizes[:-1])))\n",
    "    total_nclks = sum(group_sizes)\n",
    "    preamble += '''\n",
    "const int NGROUPS = {ngroups};\n",
    "const int NFIFOS_PER_GROUP = {fifos_per_group};\n",
    "const int FIFO_TOTAL_NCLKS = {total_nclks};\n",
    "const int FIFO_GROUP_NCLKS[] = {{ {group_sizes_csep} }} ;\n",
    "const int FIFO_GROUP_OFFSETS[] = {{ 0,{group_offsets_csep} }};\n",
    "'''.format(**locals())\n",
    "    \n",
    "    loadfun = '// FIFO loading functions'\n",
    "\n",
    "    for group_id, fifos in group_fifos.iteritems():\n",
    "        loadfun += '''\n",
    "void fdmt_load_fifos_group{group_id}(const fdmt_t* in, \n",
    "                                           fdmt_t* out) {{\n",
    "#pragma HLS ARRAY_PARTITION variable=in factor=NFIFOS_PER_GROUP cyclic\n",
    "#pragma HLS ARRAY_PARTITION variable=out factor=NFIFOS_PER_GROUP cyclic\n",
    "\n",
    "    for(int i =0; i < FIFO_GROUP_NCLKS[{group_id}]; i++) {{\n",
    "#pragma HLS PIPELINE II=1\n",
    "\n",
    "'''.format(**locals())\n",
    "    \n",
    "        for fifo_name in fifos:\n",
    "            loadfun += '        {fifo_name}_fifo.group_shift(i, in, out);\\n'.format(**locals())\n",
    "            \n",
    "        loadfun += '''\n",
    "    } // end for\n",
    "} // end fifo load\n",
    "\n",
    "'''        \n",
    "\n",
    "                \n",
    "    funcstart = '''void fdmt_process(unsigned int t, fdmt_t in[ND_IN][NC], fdmt_t out[ND]) { \n",
    "    #pragma HLS PIPELINE II=16\n",
    "'''\n",
    "    funcdecl = ''\n",
    "    funcrun = ''\n",
    "    lastiter = len(thefdmt.hist_nf_data) -1\n",
    "    for iterno, nfd in enumerate(thefdmt.hist_nf_data):\n",
    "        nextiter = iterno+1\n",
    "        out_shape = thefdmt.hist_state_shape[nextiter]\n",
    "        in_shape = thefdmt.hist_state_shape[iterno]\n",
    "        if iterno != 0:\n",
    "            funcdecl += '    fdmt_t d_iter{i}[{nd}][{nc}];\\n'.format(i=iterno,nc=in_shape[0], nd=in_shape[1])\n",
    "            \n",
    "        if iterno == 0:\n",
    "            funcrun += '    iteration{iterno}(t, in, d_iter{nextiter});\\n'.format(**locals())\n",
    "        elif iterno == lastiter:\n",
    "            funcrun += '    iteration{iterno}(t, d_iter{iterno}, out);\\n'.format(**locals())\n",
    "        else:\n",
    "            funcrun += '    iteration{iterno}(t, d_iter{iterno}, d_iter{nextiter});\\n'.format(**locals())\n",
    "\n",
    "    \n",
    "    funcend = '''\n",
    "}\n",
    "\n",
    "'''\n",
    "    \n",
    "    fileend = \"#endif\"\n",
    "    cfile = preamble  + queuedecl + iters + funcstart + funcdecl + funcrun + funcend + loadfun +fileend\n",
    "    \n",
    "    return cfile\n",
    "\n",
    "\n",
    "thefdmt = fdmt.Fdmt(f1, chanbw, 288, 1024, Nt)\n",
    "\n",
    "s = make_c_file_iter2(thefdmt, 64)\n",
    "fout = 'fdmt_d{f.max_dt}_c{f.n_f}_f{f.f_min}_iter.h'.format(f=thefdmt)\n",
    "print('Writing to ', fout)\n",
    "print(s)\n",
    "with open(fout, 'w') as fout:\n",
    "    fout.write(s)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_c_file_iter3(thefdmt, fifos_per_group=64, max_cache_depth=512,comment_constants=True):\n",
    "    ''' Splits the caches up into 16-BRAMs wide = versions'''\n",
    "    all_offsets = []\n",
    "    output_fifo_sizes = OrderedDict()\n",
    "    # make initial nodes\n",
    "    preamble = '''\n",
    "#ifndef _FDMT_PROCESS_H\n",
    "#define _FDMT_PROCESS_H\n",
    "// \n",
    "// FDMT produced by FDMT one samlpe at a time.ipynb\n",
    "// nd={f.max_dt} nf={f.n_f} fmin={f.f_min} nchan={f.n_f} df={f.d_f} bw={f.bw}\n",
    "//\n",
    "    '''\n",
    "    \n",
    "    constants = '''\n",
    "    \n",
    "// CONSTATNS - copy these to fdmt.h and uncomment them\n",
    "// TODO: make this whole thing easier with .h and .cpp files.\n",
    "const int NC={f.n_f}; // Number of channels\n",
    "const float FMIN = {f.f_min}; // Frequency of bottom channel (GHz)\n",
    "const float FMAX = {f.f_max}; // Frequency of bottom channel (GHz)\n",
    "const int ND = {f.max_dt}; // Number of output DM trials\n",
    "const int ND_IN = {f.hist_state_shape[0][1]}; // number of input dm trials\n",
    "const float DF = {f.d_f}; // channel interval (GHz)\n",
    "const float BW = {f.bw}; // Total bandwidth (GHz)\n",
    "\n",
    "'''.format(f=thefdmt)\n",
    "    \n",
    "    ishape = thefdmt.hist_state_shape[0]\n",
    "    iters = ''\n",
    "    queuedecl = ''\n",
    "    \n",
    "\n",
    "    for iterno, nfd in enumerate(thefdmt.hist_nf_data):\n",
    "        out_shape = thefdmt.hist_state_shape[iterno+1]\n",
    "        in_shape = thefdmt.hist_state_shape[iterno]\n",
    "        nchan, ndt, nt_out = out_shape\n",
    "\n",
    "        print 'Iteration {} in={} size={} out={} size={}'.format(iterno+1, in_shape[0:2], in_shape[0:2].prod(), out_shape[0:2], out_shape[:2].prod())\n",
    "        ncout, ndout = out_shape[0:2]\n",
    "        ncin, ndin = in_shape[0:2]\n",
    "        sums_done = set()\n",
    "        iterstart = ''\n",
    "        iterstart += '//Iteration {iterno}\\n'.format(**locals())\n",
    "        if ncout == 1:\n",
    "            iterstart += 'void iteration{iterno}(unsigned int t, const fdmt_t in[{ndin}][{ncin}], fdmt_t out[{ndout}]) \\n'.format(**locals())\n",
    "        else:\n",
    "            iterstart += 'void iteration{iterno}(unsigned int t, const fdmt_t in[{ndin}][{ncin}], fdmt_t out[{ndout}][{ncout}]) \\n'.format(**locals())\n",
    "\n",
    "        iterstart += '{\\n'\n",
    "        queuepush = '// FIFO push statements\\n\\n'\n",
    "        do_sums = ''\n",
    "        #do_sums += '\\n\\n// Iteration {} in={} size={} out={} size={} \\n\\n'.format(iterno+1, in_shape, in_shape[0:2].prod(), out_shape, out_shape[:2].prod())\n",
    "        read = ' '*4 + '// Read inputs\\n'\n",
    "        for c in xrange(ncin):\n",
    "            for d in xrange(ndin):\n",
    "                read += '    fdmt_t {} = in[{}][{}];\\n'.format(fmt(iterno, c, d), d,c)\n",
    "                \n",
    "\n",
    "        for ochan in xrange(nchan):\n",
    "            chanconfig = thefdmt.hist_nf_data[iterno][ochan][-1]\n",
    "            #print '\\tOut channel {}'.format(ochan)\n",
    "            last_id1 = -1\n",
    "            last_id2 = -1\n",
    "            \n",
    "            do_sums += '\\n // Output channel {}\\n'.format(ochan)\n",
    "            for idt, config in enumerate(chanconfig):\n",
    "                _, id1, offset, id2, _, _, _ = config\n",
    "                do_copy = id2 == -1\n",
    "                inchan1 = 2*ochan\n",
    "                inchan2 = inchan1+1\n",
    "\n",
    "                #print '\\t Out Channel {} idout={} from chan{}/idt{}{} + chan{}/idt{}{} with offset {}' \\\n",
    "                #    .format(ochan, idt, inchan1, id1, id1_hit, inchan2, id2, id2_hit, offset)\n",
    "\n",
    "                all_offsets.append(offset)\n",
    "\n",
    "                n1 = fmt(iterno, inchan1, id1)\n",
    "                n2 = fmt(iterno, inchan2, id2)\n",
    "                nout = fmt(iterno+1,ochan, idt)\n",
    "                \n",
    "                # For a given sum - only maintain 1 fifo of the given max length\n",
    "                ff = output_fifo_sizes.get(n2, [])\n",
    "                ff.append(offset)\n",
    "                output_fifo_sizes[n2] = ff\n",
    "                \n",
    "                \n",
    "                if do_copy:\n",
    "                    do_sums += 'fdmt_t {} = {};\\n'.format(nout, n1)\n",
    "                else:\n",
    "                    if offset == 0:\n",
    "                        do_sums += '    fdmt_t {} = {} + {};\\n'.format(nout, n1, n2)\n",
    "                    else:\n",
    "                        do_sums += '    fdmt_t {} = {} + {}_fifo.read({});\\n'.format(nout, n1, n2, offset-1)\n",
    "                        \n",
    "                sums_done.add((ochan, idt))\n",
    "                        \n",
    "\n",
    "        # Find FIFOS for this iteration\n",
    "        myfifos = filter(lambda f: f.startswith('I{}'.format(iterno)), output_fifo_sizes)\n",
    "        for infmt in myfifos:\n",
    "            ff_sizes = output_fifo_sizes[infmt]\n",
    "            #queuedecl += 'static fdmt_fifo<{},{}> {}_fifo;\\n'.format(min(ff_sizes), max(ff_sizes), infmt)\n",
    "            #queuedecl += '#pragma HLS ARRAY_PARTITION variable={}_fifo dim=1 complete\\n'.format(infmt)\n",
    "            # add 1 because of how lengths are defined. Ap_shift_reg wants size, fdmt_fifo wanted max_read_value\n",
    "            queuepush += '    {}_fifo.shift({});\\n'.format(infmt, infmt)\n",
    "\n",
    "        # write outputs\n",
    "        oshape = thefdmt.hist_state_shape[iterno+1]\n",
    "\n",
    "        write = '\\n\\n// Write outputs\\n\\n'\n",
    "        for c in xrange(ncout):\n",
    "            for d in xrange(ndout):\n",
    "                if (c, d) in sums_done: # if the sum was actually done - load it into the array\n",
    "                    vout = fmt(iterno+1, c, d);\n",
    "                else:\n",
    "                    vout = '0';  # set the few dangling outputs to zero\n",
    "                \n",
    "                if ncout == 1:                    \n",
    "                    write += '    out[{}] = {};\\n'.format(d, vout)\n",
    "                else:\n",
    "                    write += '    out[{}][{}] = {};\\n'.format(d, c , vout)\n",
    "                \n",
    "        \n",
    "        iters += iterstart + read  + do_sums + queuepush + write + '}\\n\\n'\n",
    "        \n",
    "    # sort queues by size\n",
    "    sorted_queues = sorted(output_fifo_sizes.items(), key=lambda fsz: max(fsz[1]))\n",
    "    nfifos = len(output_fifo_sizes)\n",
    "    ngroups = int(np.ceil(float(nfifos)/float(fifos_per_group)))\n",
    "    npad = ngroups*fifos_per_group - nfifos # number of FIFOS not to include in the first group\n",
    "    assert 0 <= npad < fifos_per_group\n",
    "    # NB: padding at the beginning rather than the end saves a bunch of memory\n",
    "    group_sizes = np.zeros(ngroups, dtype=int)\n",
    "    group_fifos = {}\n",
    "\n",
    "    for fifo_enum, (fifo_name, fifo_sizes) in enumerate(sorted_queues):\n",
    "        fifo_id = fifo_enum + npad # pad id from the beginning\n",
    "        group_id = fifo_id // fifos_per_group\n",
    "        group_offset = fifo_id % fifos_per_group\n",
    "        fifo_size = max(fifo_sizes)\n",
    "        queuedecl += 'static FdmtFifo<{}, {}, {}> {}_fifo;\\n'.format(fifo_size, group_id, group_offset, fifo_name);\n",
    "        group_sizes[group_id] = max(group_sizes[group_id], fifo_size)\n",
    "        gf = group_fifos.get(group_id, [])\n",
    "        gf.append(fifo_name)\n",
    "        group_fifos[group_id] = gf\n",
    "        \n",
    "\n",
    "    loadfun = '// FIFO loading functions'\n",
    "\n",
    "    cacheid = 0\n",
    "    this_cache_depth = 0\n",
    "    cache_sizes = []\n",
    "    group_offsets = []\n",
    "    group_cache_ids = []\n",
    "    for group_id, fifos in group_fifos.iteritems():\n",
    "        # Work out cache sizes\n",
    "        groupsz = group_sizes[group_id]\n",
    "        if this_cache_depth + groupsz > max_cache_depth:\n",
    "            cache_sizes.append(this_cache_depth)\n",
    "            cacheid += 1\n",
    "            this_cache_depth = 0\n",
    "            \n",
    "        group_offsets.append(this_cache_depth)\n",
    "        group_cache_ids.append(cacheid)\n",
    "        this_cache_depth += groupsz\n",
    "        \n",
    "        # make load function\n",
    "        loadfun += '''\n",
    "void fdmt_load_fifos_group{group_id}(const group_cache_t input_cache, \n",
    "                                           group_cache_t output_cache) {{\n",
    "\n",
    "    for(int i =0; i < FIFO_GROUP_NCLKS[{group_id}]; i++) {{\n",
    "#pragma HLS PIPELINE II=1\n",
    "\n",
    "'''.format(**locals())\n",
    "    \n",
    "        for fifo_name in fifos:\n",
    "            loadfun += '        {fifo_name}_fifo.group_shift(i, input_cache, output_cache);\\n'.format(**locals())\n",
    "            \n",
    "        loadfun += '''\n",
    "    } // end for\n",
    "} // end fifo load\n",
    "\n",
    "''' \n",
    "\n",
    "    # Add final cache size\n",
    "    cache_sizes.append(this_cache_depth)\n",
    "\n",
    "        \n",
    "    group_sizes_csep = ','.join(map(str, group_sizes))\n",
    "    group_offsets_csep =  ','.join(map(str, group_offsets))\n",
    "    cache_sizes_csep = ','.join(map(str, cache_sizes))\n",
    "    group_cache_ids_csep = ','.join(map(str, group_cache_ids))\n",
    "    num_caches = len(cache_sizes)\n",
    "    total_nclks = sum(group_sizes)\n",
    "    constants += '''\n",
    "const int NGROUPS = {ngroups}; // total number of FIFO groups \n",
    "const int NFIFOS_PER_GROUP = {fifos_per_group}; // Number of FIFOs in a group. Should = DRAM bus width = 64 in full system\n",
    "const int MAX_CACHE_DEPTH = {max_cache_depth}; // Maximum depth of a cache block. Is the depth of the BRAM = 1024 in full system\n",
    "const int FIFO_TOTAL_NCLKS = {total_nclks}; // Total number of CLKS require to clock in all the FIFOs serially\n",
    "const int FIFO_GROUP_NCLKS[] = {{ {group_sizes_csep} }} ; // Number of clocks to load in each FIFO group\n",
    "const int FIFO_GROUP_OFFSETS[] = {{ {group_offsets_csep} }}; // Offset address for each FIFO group within its cache block\n",
    "const int NUM_CACHES = {num_caches}; // Total number of cache blocks\n",
    "const int CACHE_SIZES[] = {{ {cache_sizes_csep} }}; // Depth of each individual cache (we can't always use all of a cache)\n",
    "const int FIFO_GROUP_CACHE_IDS = {{ {group_cache_ids_csep} }}; // The ID of the cache each group will go in\n",
    "\n",
    "'''.format(**locals())\n",
    "    \n",
    "    assert max(cache_sizes) <= max_cache_depth\n",
    "    \n",
    "\n",
    "                \n",
    "    funcstart = '''void fdmt_process(unsigned int t, fdmt_t in[ND_IN][NC], fdmt_t out[ND]) { \n",
    "    #pragma HLS PIPELINE II=16\n",
    "'''\n",
    "    funcdecl = ''\n",
    "    funcrun = ''\n",
    "    lastiter = len(thefdmt.hist_nf_data) -1\n",
    "    for iterno, nfd in enumerate(thefdmt.hist_nf_data):\n",
    "        nextiter = iterno+1\n",
    "        out_shape = thefdmt.hist_state_shape[nextiter]\n",
    "        in_shape = thefdmt.hist_state_shape[iterno]\n",
    "        if iterno != 0:\n",
    "            funcdecl += '    fdmt_t d_iter{i}[{nd}][{nc}];\\n'.format(i=iterno,nc=in_shape[0], nd=in_shape[1])\n",
    "            \n",
    "        if iterno == 0:\n",
    "            funcrun += '    iteration{iterno}(t, in, d_iter{nextiter});\\n'.format(**locals())\n",
    "        elif iterno == lastiter:\n",
    "            funcrun += '    iteration{iterno}(t, d_iter{iterno}, out);\\n'.format(**locals())\n",
    "        else:\n",
    "            funcrun += '    iteration{iterno}(t, d_iter{iterno}, d_iter{nextiter});\\n'.format(**locals())\n",
    "\n",
    "    \n",
    "    funcend = '''\n",
    "}\n",
    "\n",
    "'''\n",
    "    \n",
    "    fileend = \"#endif\"\n",
    "    \n",
    "    constants_com = ''\n",
    "    for c in constants.split('\\n'):\n",
    "        constants_com += '// ' + c.strip() + '\\n';\n",
    "        \n",
    "    if comment_constants:\n",
    "        constants = constants_com\n",
    "        \n",
    "    \n",
    "    cfile = preamble  + constants +queuedecl + iters + funcstart + funcdecl + funcrun + funcend + loadfun +fileend\n",
    "    \n",
    "    return cfile\n",
    "\n",
    "\n",
    "thefdmt = fdmt.Fdmt(f1, chanbw, 16, 16, Nt)\n",
    "\n",
    "s = make_c_file_iter3(thefdmt, 8, max_cache_depth=16)\n",
    "fout = 'fdmt_d{f.max_dt}_c{f.n_f}_f{f.f_min}_iter3.h'.format(f=thefdmt)\n",
    "print('Writing to ', fout)\n",
    "print(s)\n",
    "with open(fout, 'w') as fout:\n",
    "    fout.write(s)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nd, nchan in configs:\n",
    "    thefdmt = fdmt.Fdmt(f1, chanbw, nchan, nd, Nt)\n",
    "    s = make_c_file_iter3(thefdmt)\n",
    "    fout = 'fdmt_d{f.max_dt}_c{f.n_f}_f{f.f_min}_iter3.h'.format(f=thefdmt)\n",
    "    print('Writing to ', fout)\n",
    "    #print(s)\n",
    "    with open(fout, 'w') as fout:\n",
    "        fout.write(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do we load and restore the FIFOs?\n",
    "Overlap and sum of the FDMt output is prohibitive, as the FDMT inflates the data by 4-8 the UV plane inflates by 10 - giving a total inflation of ~50-100 bringing in and sending out blocks of data at that rate is pretty terrible (TBC). - but, what if we load and restore the FIFOs? As long as we don't cop too much overhead, and can handle the muxing, we might be better off.\n",
    "\n",
    "A few issues: the rule of FIFOs is  you can't push in more than one value per clock. So you need at least max(total_offsets) clocks to push the data into the longest FIFO. But, in practice, there's a bunch of data to bring in, so really it's sum(total_offsets) of entries - so the time it takes is the max of those two.\n",
    "\n",
    "But it's worse than that because the FIFOs are different lengths and there's a bit more data than that This is combined with the fact that a transfer is optimally 512 bits wide = 64 numbers - so ideally you'd load 64 different FIFOs per clock/transfer, but, of course, all the FIFOs are different lengths so you might not want to load some FIFOs. But - maybe you can just make them all the same length (padding some with a bit of extra) and that way you can, load 64 FIFOs at a time. The end data will be junk and the actual FDMT processing will need to make sure they read from the correct part of the FIFO.\n",
    "\n",
    "The good news is that you can push data into the FIFOs at the same time as you're saving the data from the FIFOs. So that's at least 2x. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_size_bits = 512 # DRAM bus width\n",
    "entry_bits = 8 # Bits per entry\n",
    "entries_per_transfer = float(transfer_size_bits / entry_bits)\n",
    "clocks_per_block = float(nt*max(nchan, nd)/entries_per_transfer) # max(nchan, nd) - is depending on whether it's input bound or output-bound\n",
    "transfer_clocks = sum(total_offsets)/entries_per_transfer\n",
    "longest_fifo = max(total_offsets)\n",
    "worst_case_load_nclk = float(max(longest_fifo, transfer_clocks))\n",
    "best_case_load_nclk = float(min(longest_fifo, transfer_clocks))\n",
    "\n",
    "print 'Entries per transfer', entries_per_transfer, 'processing clocks_per_block', clocks_per_block\n",
    "print 'longest_fifo', longest_fifo, 'transfer_clocks', transfer_clocks, 'worst case load nclks', worst_case_load_nclk\n",
    "print 'Worst case Processing efficiency', clocks_per_block/(worst_case_load_nclk + clocks_per_block)\n",
    "print 'Best possible efficiency', clocks_per_block/(best_case_load_nclk + clocks_per_block)\n",
    "print 'Entry cache Num BRAMS', sum(total_offsets)*8/18e3*2 # 1 for input + 1 for output\n",
    "print 'Number of entries that can be loaded from memory while clocking largest fifo', longest_fifo*entries_per_transfer, '=', longest_fifo*entries_per_transfer/float(sum(total_offsets))*100, '%'\n",
    "\n",
    "nfifos_by_size, fsize= np.histogram(total_offsets, bins=np.arange(0, max(total_offsets) + 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so roughly here's the idea. the time to load the FIFOs is dominated by the time to pull the data in from DRAM - which is about 4000 clocks (Spooky that it's so close to the size of the output) - which means the efficiency is almost exactly 50%. Fortunately this means if you can dribble the data in from DRAM to BRAM, then load it at the maximum rate from BRAM, you can get up to 87% efficiency. Ideally we'll need to overlap the pull in from DRAM at the same time as the processing. And, we'll need to double-buffer it as we need to shift into the FIFOs a the same time as shifting out of the FIFOS.\n",
    "\n",
    "OK so that's useful. We need 117 BRAMS, but there are 1344 on an SLR - so that isn't too bad. Only 13% of the data could be brought in while clocking the largest fifo, so trying to keep that 13% out of BRAM and only throwing it in a the last minute isn't really necessary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "semilogy(fsize[1:-1], nfifos_by_size[1:])\n",
    "xlabel('FIFO Size (entries)')\n",
    "ylabel('Number of FIFOs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive version of transferrs - don't group together different sizes\n",
    "ngroups_per_size = np.ceil(nfifos_by_size/float(entries_per_transfer))\n",
    "nclocks = ngroups_per_size * fsize[:-1]\n",
    "plot(fsize[:-1], ngroups_per_size)\n",
    "xlabel('FIFO Size')\n",
    "ylabel('Number of FIFO transfer groups with this size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(fsize[:-1], nclocks)\n",
    "xlabel('FIFO group num entries')\n",
    "ylabel('Numeber of clocks to load')\n",
    "print 'Total clocks to load', sum(nclocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK - so if you do the naieve thing and only group together FIFOs with identical lengths, you get completely unacceptable results = 140e3 clocks to load everything (recall the whole FDMT only takes 4096 clocks to do 256 samples).Next, why don't we pack everything into the smallest number of transfers.\n",
    "\n",
    "# What's the memory penalty if we pack FIFOs into the smallest number of transfers?\n",
    "Hint: Pre-padding blank FIFOs wastes a lot less memory than post-padding - where the large number of clocks for the largest fifofs take the most time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfifos = len(total_offsets)\n",
    "num_groups = int(np.ceil(float(nfifos)/float(entries_per_transfer))) # total number of groups of FIFOs\n",
    "npad = int(num_groups*entries_per_transfer - nfifos)\n",
    "total_offsets_padded = [0 for i in xrange(npad)]\n",
    "total_offsets_padded.extend(sorted(total_offsets))\n",
    "print total_offsets_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fifo_group_nclks = np.array(total_offsets_padded).reshape(num_groups, int(entries_per_transfer))\n",
    "print fifo_group_nclks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fifo_group_nclks.max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest_fifo_in_group = fifo_group_nclks.max(axis=1)\n",
    "smallest_fifo_in_group = fifo_group_nclks.min(axis=1)\n",
    "\n",
    "total_clocks = largest_fifo_in_group.sum()\n",
    "print 'Total clocks used:', total_clocks\n",
    "print 'Total data loaded:', total_clocks*entries_per_transfer, 'memory Overhead', total_clocks*entries_per_transfer/sum(total_offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wasted_data = (fifo_group_nclks[:, :] - smallest_fifo_in_group[:, np.newaxis]).sum(axis=1)\n",
    "\n",
    "plot(wasted_data)\n",
    "xlabel('FIFO group ID')\n",
    "ylabel('Amount of wasted clocks/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fifo_group_nclks[-2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fifo_group_nclks[-1, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
